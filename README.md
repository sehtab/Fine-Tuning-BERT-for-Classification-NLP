# Fine-Tuning-BERT-for-Classification-NLP

https://colab.research.google.com/drive/16tVY3eGSni6-xCxl7AC0qNVWasoN9eY8


# Objective:

The objective of this ICP is to spam classification with Fine-tuned BERT transformer.

# Approaches

At first, necessary libraries are imported. The dataset is uploaded then tokenized and splitted. After tokenization necessary BERT class and methods 
are defined and Fine-tuned. BERT transformer performance is compared with other classification algorithms.


# Datasets

For this ICP from dataset from kaggle are used. Spam-ham in csv format are used for BERT transformer.At last, csv format is changed to work with BERT.

# Results:

Results are generated with sklearn library with BERT and other classification algorithms..

# Challenges

For this icp, it takes a lot of time to think about BERT fine-tuning.

# Planned Work

At first, BERT library is installed and implemented and BERT tokenizer is implemented and dataset is classified. BERT performance is compared with other classification 
algorithms
![image](https://user-images.githubusercontent.com/70243598/195206875-63d801f7-9f65-4768-8df3-f1f928118bef.png)
![image](https://user-images.githubusercontent.com/70243598/195206963-7960b4d6-a94a-451d-bebf-381ddd14b03b.png)
